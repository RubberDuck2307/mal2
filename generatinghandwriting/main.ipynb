{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "X = np.load('x_letters.npy')\n",
    "y = np.load('y_letters.npy')\n",
    "\n",
    "X_tensor = tf.convert_to_tensor(X, dtype=tf.float32)\n",
    "\n",
    "y_tensor = tf.convert_to_tensor(y, dtype=tf.float32)\n",
    "\n",
    "y_flat = tf.reshape(y_tensor, [-1])\n",
    "\n",
    "vals, idx, counts = tf.unique_with_counts(y_flat)\n",
    "\n",
    "first_indices = []\n",
    "for v in vals.numpy():\n",
    "    pos = tf.where(y_flat == v)\n",
    "    first_indices.append(pos[0][0].numpy())\n",
    "\n",
    "for i, val, count in zip(first_indices, vals.numpy(), counts.numpy()):\n",
    "    print(f\"Value: {val}, Count: {count}\")\n",
    "    img = X[i]\n",
    "    #plt.imshow(img, cmap='gray')\n",
    "    # plt.axis('off')\n",
    "    # plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e02c221475602668",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import keras\n",
    "from tensorflow.keras import layers, Sequential\n",
    "import string\n",
    "\n",
    "\n",
    "@keras.saving.register_keras_serializable()\n",
    "class Descriminator(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Descriminator, self).__init__()\n",
    "        self.dense = Sequential([\n",
    "            layers.Dense(256, activation=\"relu\"),\n",
    "            layers.Dropout(0.3),\n",
    "            layers.Dense(128, activation=\"relu\"),\n",
    "            layers.Dropout(0.3),\n",
    "            layers.Dense(64, activation=\"relu\"),\n",
    "            layers.Dropout(0.3),\n",
    "        ])\n",
    "        self.out_real_classifier = layers.Dense(1)\n",
    "        self.out_label_classifier = layers.Dense(26)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = tf.reshape(x, [x.shape[0], -1])\n",
    "        x = self.dense(x)\n",
    "        reality_logits = self.out_real_classifier(x)\n",
    "        label_logits = self.out_label_classifier(x)\n",
    "        return reality_logits, label_logits\n",
    "\n",
    "    def get_config(self):\n",
    "        return {}\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls()\n",
    "\n",
    "\n",
    "@keras.saving.register_keras_serializable()\n",
    "class Generator(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.net = Sequential([\n",
    "            layers.Dense(14 * 14, activation=\"relu\"),\n",
    "            layers.Dropout(0.3),\n",
    "            layers.Dense(28 * 28, activation=\"sigmoid\"),\n",
    "            layers.Dropout(0.3),\n",
    "        ])\n",
    "\n",
    "    def call(self, noise, labels):\n",
    "        x = tf.concat([noise, labels], axis=1)\n",
    "        x = self.net(x)\n",
    "        x = tf.reshape(x, [-1, 28, 28])\n",
    "        return x\n",
    "\n",
    "\n",
    "def discriminatorLoss(real_reality_logits, fake_reality_logits, labels_logits, labels_gt):\n",
    "    bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    sce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    real_loss = bce(tf.ones_like(real_reality_logits),\n",
    "                    real_reality_logits)\n",
    "    fake_loss = bce(tf.zeros_like(fake_reality_logits),\n",
    "                    fake_reality_logits)\n",
    "    label_loss = sce(labels_gt, labels_logits)\n",
    "\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def generatorLoss(fake_logits, labels_logits, labels_gt):\n",
    "    bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    sce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    real_loss = bce(tf.ones_like(fake_logits), fake_logits)\n",
    "    label_loss = sce(labels_gt, labels_logits) * 1.1\n",
    "    total_loss = real_loss\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def generate_random_image(generator, latent_dim):\n",
    "    random_latent_vector = tf.random.normal(shape=(1, latent_dim))\n",
    "    random_label_index = tf.random.uniform(shape=(1,), minval=0, maxval=26, dtype=tf.int32)\n",
    "\n",
    "    random_label = tf.one_hot(random_label_index, depth=26)\n",
    "\n",
    "    generated_image = generator(random_latent_vector, random_label)\n",
    "\n",
    "    letter = string.ascii_uppercase[random_label_index[0].numpy()]\n",
    "\n",
    "    plt.imshow(generated_image[0], cmap='gray')\n",
    "    plt.title(f\"Generated Letter: {letter}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def generate_all_letters(generator, latent_dim, img_size=(28, 28)):\n",
    "    num_letters = 26\n",
    "    plt.figure(figsize=(22, 20))\n",
    "\n",
    "    for i, letter in enumerate(string.ascii_uppercase):\n",
    "        latent_vector = tf.random.normal(shape=(1, latent_dim))\n",
    "        label = tf.one_hot([i], depth=26)\n",
    "        generated_image = generator(latent_vector, label)\n",
    "\n",
    "        plt.subplot(1, num_letters, i + 1)\n",
    "        plt.imshow(generated_image[0], cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.title(letter, fontsize=8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7420f87d99706d75",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from tensorflow.keras import layers, optimizers\n",
    "import keras\n",
    "from keras import ops\n",
    "import os\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "\n",
    "def train_d(batch_size, real_images, real_labels, discriminator, generator, d_optimizer, latent_dim):\n",
    "    random_latent_vectors = keras.random.normal(\n",
    "        shape=(batch_size, latent_dim)\n",
    "    )\n",
    "\n",
    "    random_label_indices = tf.random.uniform(\n",
    "        shape=(batch_size,),\n",
    "        minval=0,\n",
    "        maxval=26,\n",
    "        dtype=tf.int32\n",
    "    )\n",
    "\n",
    "    random_labels = tf.one_hot(random_label_indices, depth=26)\n",
    "\n",
    "    generated_images = generator(random_latent_vectors, random_labels)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        all_images = tf.concat([real_images, generated_images], axis=0)\n",
    "\n",
    "        all_reality_logits, all_label_logits = discriminator(all_images, training=True)\n",
    "\n",
    "        reality_real_logits, reality_generated_logits = tf.split(all_reality_logits,\n",
    "                                                                 num_or_size_splits=[batch_size, batch_size],\n",
    "                                                                 axis=0)\n",
    "        reality_label_logits, label_generated_logits = tf.split(all_label_logits,\n",
    "                                                                num_or_size_splits=[batch_size, batch_size],\n",
    "                                                                axis=0)\n",
    "        loss = discriminatorLoss(reality_real_logits, reality_generated_logits, reality_label_logits,\n",
    "                                 real_labels)\n",
    "        grads = tape.gradient(loss, discriminator.trainable_variables)\n",
    "        d_optimizer.apply_gradients(zip(grads, discriminator.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def train_g(batch_size, discriminator, generator, g_optimizer, latent_dim, real_images):\n",
    "    random_latent_vectors = keras.random.normal(\n",
    "        shape=(batch_size, latent_dim)\n",
    "    )\n",
    "\n",
    "    random_label_indices = tf.random.uniform(\n",
    "        shape=(batch_size,),\n",
    "        minval=0,\n",
    "        maxval=26,\n",
    "        dtype=tf.int32\n",
    "    )\n",
    "\n",
    "    random_labels = tf.one_hot(random_label_indices, depth=26)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        reality_generated_logits, label_genarated_logits = discriminator(\n",
    "            generator(random_latent_vectors, random_labels, training=True), training=False)\n",
    "        g_loss = generatorLoss(reality_generated_logits, label_genarated_logits, random_label_indices)\n",
    "    grads = tape.gradient(g_loss, generator.trainable_weights)\n",
    "    g_optimizer.apply_gradients(zip(grads, generator.trainable_weights))\n",
    "\n",
    "    return g_loss\n",
    "\n",
    "\n",
    "def should_restart(last_loss, current_loss, threshold=0.01, max_epochs_no_improve=5, current_epoch_no_improve=0):\n",
    "    improvement = last_loss - current_loss\n",
    "    if improvement > threshold:\n",
    "        current_epoch_no_improve = 0\n",
    "    else:\n",
    "        current_epoch_no_improve += 1\n",
    "\n",
    "    if current_epoch_no_improve >= max_epochs_no_improve:\n",
    "        return True, current_epoch_no_improve\n",
    "    else:\n",
    "        return False, current_epoch_no_improve\n",
    "\n",
    "\n",
    "def save_discriminator(discriminator):\n",
    "    discriminator.save('best_discriminator_model.keras')\n",
    "    print(\"Saving discriminator and optimizer to memory...\")\n",
    "\n",
    "\n",
    "def save_generator(generator):\n",
    "    generator.save('best_generator_model.keras')\n",
    "    print(\"Saving generator and optimizer to memory...\")\n",
    "\n",
    "\n",
    "def load_discriminator():\n",
    "    discriminator = tf.keras.models.load_model('best_discriminator_model.keras')\n",
    "    return discriminator\n",
    "\n",
    "\n",
    "def load_generator():\n",
    "    generator = tf.keras.models.load_model('best_generator_model.keras')\n",
    "    return generator\n",
    "\n",
    "\n",
    "discriminator = Descriminator()\n",
    "d_optimizer = optimizers.Adam(learning_rate=1e-4)\n",
    "generator = Generator()\n",
    "g_optimizer = optimizers.Adam(learning_rate=1e-4)\n",
    "\n",
    "latent_dim = 100\n",
    "d_LOSS_DELTA = 0.01\n",
    "g_LOSS_DELTA = 0.01\n",
    "d_min_loss = 2.0\n",
    "g_min_loss = 2.0\n",
    "max_iter_d = 500\n",
    "max_iter_g = 1000\n",
    "cycle = 500000\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_tensor, y_tensor))\n",
    "print(len(dataset))\n",
    "dataset = dataset.shuffle(buffer_size=1024)\n",
    "dataset = dataset.batch(30000)\n",
    "dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "reload_g = False\n",
    "reload_d = False\n",
    "skip_d = False\n"
   ],
   "id": "df95ec05c8d8b12",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for c in range(cycle):\n",
    "    g_loss = 0\n",
    "    d_loss = 0\n",
    "    d_epoch = 0\n",
    "    g_epoch = 0\n",
    "    while True:\n",
    "        d_epoch += 1\n",
    "        for real_images, real_labels in dataset:\n",
    "            batch_size = ops.shape(real_images)[0]\n",
    "            d_loss = train_d(batch_size, real_images, real_labels,\n",
    "                                 discriminator, generator, d_optimizer, latent_dim).numpy()\n",
    "            if d_loss < d_min_loss:\n",
    "                break\n",
    "        if d_loss < d_min_loss:\n",
    "            break\n",
    "\n",
    "        if d_epoch % 100 == 0:\n",
    "            print(\"Still training discriminator, epoch:\", d_epoch, \", d_loss:\", d_loss)\n",
    "\n",
    "\n",
    "    while True:\n",
    "        g_epoch += 1\n",
    "        for real_images, real_labels in dataset:\n",
    "            batch_size = ops.shape(real_images)[0]\n",
    "            g_loss = train_g(batch_size, discriminator, generator, g_optimizer, latent_dim).numpy()\n",
    "\n",
    "            if g_loss < g_min_loss:\n",
    "                break\n",
    "        if g_loss < g_min_loss:\n",
    "            break\n",
    "\n",
    "        if g_epoch % 100 == 0:\n",
    "            print(\"Still training generator, epoch:\", g_epoch, \", g_loss:\", g_loss)\n",
    "    if c % 1 == 0:\n",
    "        print(\"epoch:\", g_epoch, \", g_loss:\", g_loss, \"d_loss:\", d_loss)\n",
    "        save_discriminator(discriminator)\n",
    "        save_generator(generator)\n",
    "        generate_all_letters(generator, latent_dim)\n",
    "\n",
    "    d_min_loss = d_min_loss - d_min_loss * 0.05\n",
    "    g_min_loss = g_min_loss - g_min_loss * 0.05\n"
   ],
   "id": "90730415274219f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "\n",
    "first_run = True\n",
    "for c in range(cycle):\n",
    "    g_loss = 0\n",
    "    d_loss = 0\n",
    "    d_epoch = 0\n",
    "    g_epoch = 0\n",
    "\n",
    "    for real_images, real_labels in dataset:\n",
    "        batch_size = ops.shape(real_images)[0]\n",
    "        num = random.randint(1, 100)\n",
    "        if num < 70:\n",
    "            d_loss = train_d(batch_size, real_images, real_labels,\n",
    "                             discriminator, generator, d_optimizer, latent_dim).numpy()\n",
    "        g_loss = train_g(batch_size, discriminator, generator, g_optimizer, latent_dim).numpy()\n",
    "\n",
    "    if c % 1 == 0:\n",
    "        print(\"epoch:\", g_epoch, \", g_loss:\", g_loss, \"d_loss:\", d_loss)\n",
    "        save_discriminator(discriminator)\n",
    "        save_generator(generator)\n",
    "        generate_all_letters(generator, latent_dim)\n",
    "\n",
    "    d_min_loss = d_min_loss - d_min_loss * 0.05\n",
    "    g_min_loss = g_min_loss - g_min_loss * 0.05\n"
   ],
   "id": "bc5fb67f0e33cc8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import string\n",
    "\n",
    "letters = list(string.ascii_uppercase)\n",
    "\n",
    "for i, val, count in zip(first_indices, vals.numpy(), counts.numpy()):\n",
    "    print(f\"Value: {val}, Count: {count}\")\n",
    "    img = X[i]\n",
    "\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    val_batch = tf.expand_dims(img, axis=0)\n",
    "\n",
    "    reality_logits, label_logits = discriminator(val_batch)\n",
    "\n",
    "    print(\"Reality logits:\", reality_logits.numpy())\n",
    "\n",
    "    probs = tf.nn.softmax(label_logits)  # convert logits to probabilities\n",
    "    top3 = tf.nn.top_k(probs, k=3)\n",
    "\n",
    "    top_letters = [letters[idx] for idx in top3.indices.numpy()[0]]\n",
    "    top_probs = top3.values.numpy()[0]\n",
    "\n",
    "    print(\"Top 3 predicted letters:\")\n",
    "    for letter, prob in zip(top_letters, top_probs):\n",
    "        print(f\"{letter}: {prob:.4f}\")\n",
    "\n",
    "    if i > 5:\n",
    "        break"
   ],
   "id": "4ddd48a3896dca8c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": false
   },
   "cell_type": "code",
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "from keras import ops\n",
    "\n",
    "\n",
    "class GAN(keras.Model):\n",
    "    def __init__(self, discriminator, generator, latent_dim):\n",
    "        super().__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "        self.seed_generator = keras.random.SeedGenerator(1337)\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, loss_fn_discriminator, loss_fn_generator):\n",
    "        super().compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.loss_fn_generator = loss_fn_generator\n",
    "        self.loss_fn_discriminator = loss_fn_discriminator\n",
    "        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n",
    "        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.d_loss_metric, self.g_loss_metric]\n",
    "\n",
    "    def train_step(self, real_images, real_labels, discriminator, generator):\n",
    "        batch_size = ops.shape(real_images)[0]\n",
    "\n",
    "        if discriminator:\n",
    "            random_latent_vectors = keras.random.normal(\n",
    "                shape=(batch_size, self.latent_dim)\n",
    "            )\n",
    "\n",
    "            random_label_indices = tf.random.uniform(\n",
    "                shape=(batch_size,),\n",
    "                minval=0,\n",
    "                maxval=26,\n",
    "                dtype=tf.int32\n",
    "            )\n",
    "\n",
    "            random_labels = tf.one_hot(random_label_indices, depth=26)\n",
    "\n",
    "            generated_images = self.generator(random_latent_vectors, random_labels)\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                all_images = tf.concat([real_images, generated_images], axis=0)\n",
    "\n",
    "                all_reality_logits, all_label_logits = self.discriminator(all_images, training=True)\n",
    "\n",
    "                reality_generated_logits, reality_real_logits = tf.split(all_reality_logits,\n",
    "                                                                         num_or_size_splits=[batch_size, batch_size],\n",
    "                                                                         axis=0)\n",
    "                label_generated_logits, reality_label_logits = tf.split(all_label_logits,\n",
    "                                                                        num_or_size_splits=[batch_size, batch_size],\n",
    "                                                                        axis=0)\n",
    "                d_loss = self.loss_fn_discriminator(reality_real_logits, reality_generated_logits, reality_label_logits,\n",
    "                                                    real_labels)\n",
    "                grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
    "                self.d_optimizer.apply_gradients(zip(grads, self.discriminator.trainable_weights))\n",
    "                for g, w in zip(grads, self.discriminator.trainable_weights):\n",
    "                    print(w.name, g is None)\n",
    "\n",
    "        if generator:\n",
    "            random_latent_vectors = keras.random.normal(\n",
    "                shape=(batch_size, self.latent_dim)\n",
    "            )\n",
    "\n",
    "            random_label_indices = tf.random.uniform(\n",
    "                shape=(batch_size,),\n",
    "                minval=0,\n",
    "                maxval=26,\n",
    "                dtype=tf.int32\n",
    "            )\n",
    "\n",
    "            random_labels = tf.one_hot(random_label_indices, depth=26)\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                reality_generated_logits, label_genarated_logits = self.discriminator(\n",
    "                    self.generator(random_latent_vectors, random_labels))\n",
    "                g_loss = self.loss_fn_generator(reality_generated_logits, label_genarated_logits, random_label_indices)\n",
    "            grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
    "            self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
    "\n",
    "        self.d_loss_metric.update_state(d_loss if discriminator else 0)\n",
    "        # self.g_loss_metric.update_state(g_loss if generator else 0)\n",
    "        return {\n",
    "            \"d_loss\": self.d_loss_metric.result(),\n",
    "            # \"g_loss\": self.g_loss_metric.result(),\n",
    "        }\n"
   ],
   "id": "dc8be6362025c75f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "feb4630dae87082",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
