{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "X = np.load('x_letters.npy')\n",
    "y = np.load('y_letters.npy')\n",
    "\n",
    "X_tensor = tf.convert_to_tensor(X, dtype=tf.float32)\n",
    "\n",
    "y_tensor = tf.convert_to_tensor(y, dtype=tf.float32)\n",
    "\n",
    "y_flat = tf.reshape(y_tensor, [-1])\n",
    "\n",
    "vals, idx, counts = tf.unique_with_counts(y_flat)\n",
    "\n",
    "first_indices = []\n",
    "for v in vals.numpy():\n",
    "    pos = tf.where(y_flat == v)\n",
    "    first_indices.append(pos[0][0].numpy())\n",
    "\n",
    "# for i, val, count in zip(first_indices, vals.numpy(), counts.numpy()):\n",
    "#     print(f\"Value: {val}, Count: {count}\")\n",
    "#     img = X[i]\n",
    "#     #plt.imshow(img, cmap='gray')\n",
    "#     plt.axis('off')\n",
    "#     plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-10-03T07:43:32.770232Z",
     "start_time": "2025-10-03T07:43:32.553315Z"
    }
   },
   "id": "e02c221475602668",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, Sequential\n",
    "\n",
    "\n",
    "class Descriminator(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Descriminator, self).__init__()\n",
    "        self.dense = layers.Dense(256, activation=\"relu\")\n",
    "        self.out_real_classifier = layers.Dense(1)\n",
    "        self.out_label_classifier = layers.Dense(26)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.dense(x)\n",
    "        fake_logits = self.out_real_classifier(x)\n",
    "        label_logits = self.out_label_classifier(x)\n",
    "        return fake_logits, label_logits\n",
    "\n",
    "\n",
    "class Generator(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.net = Sequential([\n",
    "            layers.Dense(14 * 14, activation=\"relu\"),\n",
    "            layers.Dense(28 * 28, activation=\"sigmoid\")])\n",
    "\n",
    "    def call(self, noise, labels):\n",
    "        x = tf.concat([noise, labels], axis=1)\n",
    "        x = self.net(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def getDescriminatorLoss(real_reality_logits, fake_reality_logits, labels_logits, labels_gt):\n",
    "    bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    sce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    real_loss = bce(tf.ones_like(real_reality_logits), real_reality_logits)\n",
    "    fake_loss = bce(tf.zeros_like(fake_reality_logits), fake_reality_logits)\n",
    "    label_loss = sce(labels_gt, labels_logits)\n",
    "\n",
    "    total_loss = real_loss + fake_loss + label_loss\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def getGeneratorLoss(fake_logits, labels_logits, labels_gt):\n",
    "    bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    sce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    real_loss = bce(tf.ones_like(fake_logits), fake_logits)\n",
    "    label_loss = sce(labels_gt, labels_logits)\n",
    "\n",
    "    total_loss = real_loss + label_loss\n",
    "    return total_loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-10-03T07:43:41.485502Z",
     "start_time": "2025-10-03T07:43:41.476049Z"
    }
   },
   "id": "7420f87d99706d75",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "from keras import ops\n",
    "\n",
    "\n",
    "class GAN(keras.Model):\n",
    "    def __init__(self, discriminator, generator, latent_dim):\n",
    "        super().__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "        self.seed_generator = keras.random.SeedGenerator(1337)\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, loss_fn_discriminator, loss_fn_generator):\n",
    "        super().compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.loss_fn_generator = loss_fn_generator\n",
    "        self.loss_fn_discriminator = loss_fn_discriminator\n",
    "        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n",
    "        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.d_loss_metric, self.g_loss_metric]\n",
    "\n",
    "    def train_step(self, real_images):\n",
    "        #sample random points in the latent space\n",
    "        batch_size = ops.shape(real_images)[0]\n",
    "        random_latent_vectors = keras.random.normal(\n",
    "            shape=(batch_size, self.latent_dim)\n",
    "        )\n",
    "\n",
    "        generated_images = self.generator(random_latent_vectors)\n",
    "\n",
    "        combined_images = ops.concatenate([generated_images, real_images], axis=0)\n",
    "\n",
    "        labels = ops.concatenate(\n",
    "            [ops.ones((batch_size, 1)), ops.zeros((batch_size, 1))], axis=0\n",
    "        )\n",
    "\n",
    "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            reality_logits, label_logits = self.discriminator(combined_images)\n",
    "            d_loss = self.loss_fn_discriminator(labels, predictions)       \n",
    "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
    "        self.d_optimizer.apply_gradients(zip(grads, self.discriminator.trainable_weights))\n",
    "\n",
    "        #sample random points in latent space\n",
    "        random_latent_vectors = keras.random.normal(\n",
    "            shape=(batch_size, self.latent_dim)\n",
    "        )\n",
    "\n",
    "        #misleading labels saying \"all real\"\n",
    "        misleading_labels = ops.zeros((batch_size, 1))\n",
    "\n",
    "        #train generator (don't touch the weights of the discriminator)\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(self.generator(random_latent_vectors))\n",
    "            g_loss = self.loss_fn_generator(misleading_labels, predictions)\n",
    "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
    "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
    "\n",
    "        #update metrics\n",
    "        self.d_loss_metric.update_state(d_loss)\n",
    "        self.g_loss_metric.update_state(g_loss)\n",
    "        return {\n",
    "            \"d_loss\": self.d_loss_metric.result(),\n",
    "            \"g_loss\": self.g_loss_metric.result(),\n",
    "        }\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-10-03T08:26:45.860377Z",
     "start_time": "2025-10-03T08:26:45.845828Z"
    }
   },
   "id": "d63a907e0a087e12",
   "execution_count": 12
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
